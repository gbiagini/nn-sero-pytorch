{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rf-sero.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDKTGlbqnwLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e869d77-bb34-46d6-9ffb-4c7caae21aff"
      },
      "source": [
        "from google.colab import drive\n",
        "from fastai.imports import *\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/Colab Notebooks/dev/\"\n",
        "base_dir = root_dir + 'nn-sero-pytorch/randomforest/'\n",
        "NN_dir = root_dir + 'nn-sero-pytorch'\n",
        "path = Path(base_dir)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2d81b2ab4c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/gdrive/My Drive/Colab Notebooks/dev/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'nn-sero-pytorch/randomforest/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "NN_dir = os.path.dirname(os.path.abspath('.'))\n",
        "base_dir = os.path.join(NN_dir, 'randomforest/')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmQviheFLszE"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import math\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "def metrics(print_all='no'):\n",
        "    loci = ['A', 'B', 'C', 'DPB1', 'DQB1', 'DRB1']\n",
        "    #loci = ['A']\n",
        "\n",
        "    # function to check if value can be an integer - to eliminate excess characters from serology labels\n",
        "    def checkInt(x):\n",
        "        try:\n",
        "            int(x)\n",
        "            return True\n",
        "        except ValueError:\n",
        "            return False\n",
        "\n",
        "    concordances = {}\n",
        "\n",
        "    for loc in loci:\n",
        "        newDict = {}\n",
        "        simDict = {}\n",
        "        diffDict = {}\n",
        "        oldPredict = {}\n",
        "        newPredict = {}\n",
        "        oldPredFile = NN_dir + \"/old-predictions/\" + loc + \".chile\"\n",
        "        newPreds = pd.read_csv(base_dir + \"predictions/\" + loc + \"_predictions.csv\")\n",
        "        newPreds = newPreds.set_index('allele')\n",
        "        newPreds = newPreds.to_dict()\n",
        "        newPredict = newPreds[\"serology\"]\n",
        "        for nKey in newPredict.keys():\n",
        "            adjustMe = str(newPredict[nKey])\n",
        "            adjustMe = adjustMe.replace('[','')\n",
        "            adjustMe = adjustMe.replace(']','')\n",
        "            adjustMe = adjustMe.replace('a','')\n",
        "            adjustMe = adjustMe.replace(\"'\",'')\n",
        "            adjustMe = adjustMe.split(' ')\n",
        "            newPredict[nKey] = [x.strip('a') for x in adjustMe if checkInt(x)]\n",
        "        with open(oldPredFile, \"r\") as handle:\n",
        "            for line in handle:\n",
        "                if line.find('%') == -1:\n",
        "                    next\n",
        "                else:\n",
        "                    line = line.split()\n",
        "                    if line == []:\n",
        "                        next\n",
        "                    else:\n",
        "                        line[:] = [x for x in line if (x != '[100.00%]')]\n",
        "                        allele = loc + \"*\" + str(line[0][:-1])\n",
        "                        oldPredict[allele] = line[1:]\n",
        "\n",
        "\n",
        "        for each in oldPredict.keys():\n",
        "            allDict = {}\n",
        "            allDict[\"Allele\"] = each\n",
        "            allDict[\"Old Assignment\"] = oldPredict[each]\n",
        "            if each not in newPredict.keys():\n",
        "                next\n",
        "            else:\n",
        "                allDict[\"New Assignment\"] = newPredict[each]\n",
        "                if set(newPredict[each]) != set(oldPredict[each]):\n",
        "                    diffDict[each] = allDict\n",
        "                elif set(newPredict[each]) == set(oldPredict[each]):\n",
        "                    simDict[each] = allDict\n",
        "        diffFrame = pd.DataFrame.from_dict(diffDict)\n",
        "        diffFrame = diffFrame.transpose()\n",
        "        diffFrame.to_csv(base_dir + \"comparison/\" + loc + \"_compfile.csv\", index=False)\n",
        "        simFrame = pd.DataFrame.from_dict(simDict)\n",
        "        simFrame = simFrame.transpose()\n",
        "        simFrame.to_csv(base_dir + \"comparison/\" + loc + \"_similar.csv\", index=False)\n",
        "        \n",
        "\n",
        "        for allele in newPredict.keys():\n",
        "            allDict = {}\n",
        "            allDict[\"Allele\"] = allele\n",
        "            allDict[\"Serologic Assignment\"] = newPredict[allele]\n",
        "            if allele not in oldPredict.keys():\n",
        "                newDict[allele] = allDict\n",
        "        newFrame = pd.DataFrame.from_dict(simDict)\n",
        "        newFrame = newFrame.transpose()\n",
        "        newFrame.to_csv(base_dir + \"comparison/\" + loc + \"_newsies.csv\", index=False)\n",
        "\n",
        "        simLen = len(simFrame)\n",
        "        diffLen = len(diffFrame)\n",
        "        with open(base_dir + \"comparison/\" + loc + \"_concordance.txt\", \"w+\") as fhandle:\n",
        "            fhandle.write(\"HLA-\" +loc+ \" Similar: \" + str(simLen))\n",
        "            fhandle.write(\"HLA-\" +loc+ \" Different: \" + str(diffLen))\n",
        "            concordance = (simLen / (simLen + diffLen)) * 100\n",
        "            concordances[loc] = concordance\n",
        "            fhandle.write(\"HLA-\" +loc+ \" Concordance: \" + str(concordance) + \"%\")\n",
        "            if print_all == \"yes\":\n",
        "                print(\"HLA-\" +loc+ \" Similar: \" + str(simLen))\n",
        "                print(\"HLA-\" +loc+ \" Different: \" + str(diffLen))\n",
        "                print(\"HLA-\" +loc+ \" Concordance: \" + str(concordance) + \"%\")\n",
        "    return concordances\n",
        "\n",
        "#main(print_all=\"yes\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OE6SumgONm5"
      },
      "source": [
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "def one_hot_decode(df):\n",
        "\tdf['serology']=''\n",
        "\n",
        "\tfor col in df.columns:\n",
        "\t\tdf.loc[df[col]==1,'serology'] = df['serology']+col+';'\n",
        "\n",
        "\treturn df\n",
        "\n",
        "def fix_data(uniques, data, loc, iset, ident):\n",
        "  sero = {}\n",
        "  for row in data.itertuples(name='Pandas'):\n",
        "    sero[row.allele] = str(row.serology)\n",
        "    #sero[row[1]] = str(row[-1])\n",
        "\t\n",
        "  data = data.drop('serology', axis=1)\n",
        "\n",
        "  for key in sero.keys():\n",
        "    '''\n",
        "  \t# not applicable for old_sets train/test\n",
        "    if (sero[key].find(';') != -1):\n",
        "      sero[key] = sero[key].replace('a','')\n",
        "      sero[key] = sero[key].split(';')\n",
        "    else:\n",
        "      sero[key] = sero[key].replace('a','')\n",
        "      sero[key] = [sero[key]]\n",
        "    '''\n",
        "\n",
        "    #for old_sets train/test\n",
        "    sero[key] = sero[key].split(' ')\n",
        "    \n",
        "    for x in sero[key]:\n",
        "      if (x not in uniques):\n",
        "        uniques.append(x)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  uniques = list(map(int, uniques))\n",
        "  uniques.sort()\n",
        "  uniques = list(map(str, uniques))\n",
        "  \n",
        "  for y in uniques:\n",
        "    data[y] = 0\n",
        "\n",
        "  one_sero = {}\n",
        "  for key in sero.keys():\n",
        "    one_sero[key] = { some_key : (\"1\" if (some_key in sero[key]) else \"0\")\n",
        "\t\t                  for some_key in uniques }\n",
        "  one_df = pd.DataFrame.from_dict(one_sero)\n",
        "  one_df = one_df.transpose()\n",
        "  one_df.index.name = \"allele\"\n",
        "  data = data.set_index('allele')\n",
        "  data.update(one_df, overwrite=True)\n",
        "  data.to_csv(base_dir + 'randfor/'+iset+'/'+loc+'_'+ident+'.csv', index=True)\n",
        "  return data, uniques\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFXrjuPaKmln",
        "outputId": "e4223c0e-8372-4de7-c960-28f1f405dfb1"
      },
      "source": [
        "RSEED = 0\n",
        "\n",
        "pre_concord = metrics()\n",
        "\n",
        "loci = [\"A\", \"B\", \"C\", \"DQB1\", \"DRB1\"]\n",
        "print(\"\\nPredicting...\")\n",
        "for loc in tqdm(loci):\n",
        "  uniques = []\n",
        "  print('\\n'+loc)\n",
        "  #features = pd.read_csv(base_dir + \"training/\" + loc + \"_train.csv\")\n",
        "  features = pd.read_csv(base_dir + \"park/park0216/training/\" + loc + \"_train.csv\")\n",
        "  features['serology'] = features['serology'].apply(lambda x: x.replace('a','').replace(';',' '))\n",
        "  features, sers = fix_data(uniques, features,loc,iset='training',ident='train')\n",
        "  #vfeatures = pd.read_csv(base_dir + \"training/\" + loc + \"_validation.csv\")\n",
        "  vfeatures = pd.read_csv(base_dir + \"park/park0216/training/\" + loc + \"_validation.csv\")\n",
        "  vfeatures['serology'] = vfeatures['serology'].apply(lambda x: x.replace('a','').replace(';',' '))\n",
        "  vfeatures, vsers = fix_data(uniques, vfeatures,loc,iset='training',ident='validation')\n",
        "  #test = pd.read_csv(base_dir + \"testing/\" + loc + \"_test.csv\")\n",
        "  test = pd.read_csv(base_dir + \"park/park0216/testing/\" + loc + \"_test.csv\")\n",
        "  test = test.drop('serology', axis=1)\n",
        "\n",
        "  #remove uncategorized C*12+ alleles\n",
        "  if loc == \"C\":\n",
        "    Cdrops = [\"C*12\", \"C*14\", \"C*15\", \"C*16\", \"C*17\", \"C*18\"]\n",
        "    test = test.drop(test[test.allele.str.contains('|'.join(Cdrops))].index)\n",
        "  test.to_csv(base_dir + 'randfor/testing/'+loc+'_test.csv', index=True)\n",
        "\n",
        "  features = features.append(vfeatures)\n",
        "  labels = np.array(features[sers])\n",
        "  features = features.drop(sers, axis=1)\n",
        "  features = features.reset_index()\n",
        "  indices = features[\"allele\"]\n",
        "  indices = list(indices)\n",
        "  features = features.drop('allele', axis=1)\n",
        "  feature_list = list(features.columns)\n",
        "  n_features = len(feature_list)\n",
        "  maxfeat = int(math.sqrt(n_features))+3\n",
        "\n",
        "  features = np.array(features)\n",
        "  labels[labels!=labels]='0'\n",
        "  features[features!=features]='0'\n",
        "  features = features.astype(np.int)\n",
        "  labels = labels.astype(np.int)\n",
        "\n",
        "  test_idcs = test['allele']\n",
        "  test = test.drop('allele', axis=1)\n",
        "  test_list = list(test.columns)\n",
        "  test = np.array(test)\n",
        "  test[test!=test]='0'\n",
        "  test = test.astype(np.int)\n",
        "\n",
        "  forest = RandomForestClassifier(n_estimators=550, bootstrap=False, max_features=maxfeat, n_jobs=-1,   random_state=RSEED)\n",
        "  multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)\n",
        "  multi_target_forest.fit(features,labels)\n",
        "  predictions = multi_target_forest.predict(test)\n",
        "\n",
        "  ind_labels = [str(x) for x in sers]\n",
        "  #explainer = lime.lime_tabular.LimeTabularExplainer(features,feature_names=feature_list,class_names=ind_labels,kernel_width=5)\n",
        "  #for rowexp in range(0,2):\n",
        "    #exp = explainer.explain_instance(test[rowexp], multi_target_forest.predict_proba, num_features=maxfeat)\n",
        "    #exp.show_in_notebook(show_table=True)\n",
        "\n",
        "  preds_output = pd.DataFrame(predictions, index=test_idcs, columns=ind_labels)\n",
        "  preds_output = one_hot_decode(preds_output)\n",
        "  preds_output = preds_output.drop(ind_labels, axis=1)\n",
        "  preds_output.index.name = 'allele'\n",
        "  preds_output = preds_output.apply(lambda x: str((' '.join(x['serology'].split(';')))[:-1]), result_type='broadcast', axis=1)\n",
        "  #print(preds_output)\n",
        "  preds_output.to_csv(base_dir + 'predictions/'+loc+'_predictions.csv', index=True)\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "Predicting...\n",
            "\n",
            "A\n",
            " 20%|██        | 1/5 [00:09<00:38,  9.72s/it]\n",
            "B\n",
            " 40%|████      | 2/5 [00:34<00:56, 18.77s/it]\n",
            "C\n",
            " 60%|██████    | 3/5 [00:40<00:25, 12.92s/it]\n",
            "DQB1\n",
            " 80%|████████  | 4/5 [00:45<00:09,  9.66s/it]\n",
            "DRB1\n",
            "100%|██████████| 5/5 [00:54<00:00, 10.95s/it]Done.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ggEURdcKyWC",
        "outputId": "f088bfbe-f380-4b41-8363-55bbc0c02d13"
      },
      "source": [
        "post_concord = metrics()\n",
        "\n",
        "for loc in loci:\n",
        "\tprint(loc + \" Concordance:\\t\\t\\t\\t\" + str(post_concord[loc])[:5] + \"%\")\n",
        "\tchange = post_concord[loc] - pre_concord[loc]\n",
        "\tprint(\"% Change:\\t\\t\\t\\t\" + str(change)[:5] + \"%\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Concordance:\t\t\t\t97.74%\n% Change:\t\t\t\t0.0%\nB Concordance:\t\t\t\t90.44%\n% Change:\t\t\t\t0.0%\nC Concordance:\t\t\t\t98.64%\n% Change:\t\t\t\t0.0%\nDQB1 Concordance:\t\t\t\t99.27%\n% Change:\t\t\t\t0.0%\nDRB1 Concordance:\t\t\t\t95.11%\n% Change:\t\t\t\t0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjeCUnkq02IY"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDKTGlbqnwLg",
        "outputId": "03e8aeb8-388c-4938-e612-ce41b71b82e8"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "root_dir = '../'\n",
        "base_dir = root_dir + 'randomforest/'\n",
        "path = Path(base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MmQviheFLszE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import math\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from tqdm import tqdm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "def metrics(print_all='no'):\n",
        "    loci = ['A', 'B', 'C', 'DQB1', 'DRB1']\n",
        "    #loci = ['A']\n",
        "\n",
        "    NN_dir = '../'\n",
        "\n",
        "    # function to check if value can be an integer - to eliminate excess characters from serology labels\n",
        "    def checkInt(x):\n",
        "        try:\n",
        "            int(x)\n",
        "            return True\n",
        "        except ValueError:\n",
        "            return False\n",
        "\n",
        "    concordances = {}\n",
        "\n",
        "    for loc in loci:\n",
        "        newDict = {}\n",
        "        simDict = {}\n",
        "        diffDict = {}\n",
        "        oldPredict = {}\n",
        "        newPredict = {}\n",
        "        oldPredFile = Path(NN_dir + \"old-predictions/\" + loc + \".chile\")\n",
        "        newPreds = pd.read_csv(base_dir + \"predictions/\" + loc + \"_predictions.csv\")\n",
        "        newPreds = newPreds.set_index('allele')\n",
        "        newPreds = newPreds.to_dict()\n",
        "        newPredict = newPreds[\"serology\"]\n",
        "        for nKey in newPredict.keys():\n",
        "            adjustMe = newPredict[nKey]\n",
        "            adjustMe = adjustMe.replace('[','')\n",
        "            adjustMe = adjustMe.replace(']','')\n",
        "            adjustMe = adjustMe.replace(' ','')\n",
        "            adjustMe = adjustMe.replace(\"'\",'')\n",
        "            adjustMe = adjustMe.split(',')\n",
        "            newPredict[nKey] = [x.strip('a') for x in adjustMe if checkInt(x)]\n",
        "        with open(oldPredFile, \"r\") as handle:\n",
        "            for line in handle:\n",
        "                if line.find('%') == -1:\n",
        "                    next\n",
        "                else:\n",
        "                    line = line.split()\n",
        "                    if line == []:\n",
        "                        next\n",
        "                    else:\n",
        "                        line[:] = [x for x in line if (x != '[100.00%]')]\n",
        "                        allele = loc + \"*\" + str(line[0][:-1])\n",
        "                        oldPredict[allele] = line[1:]\n",
        "\n",
        "\n",
        "        for each in oldPredict.keys():\n",
        "            allDict = {}\n",
        "            allDict[\"Allele\"] = each\n",
        "            allDict[\"Old Assignment\"] = oldPredict[each]\n",
        "            if each not in newPredict.keys():\n",
        "                next\n",
        "            else:\n",
        "                allDict[\"New Assignment\"] = newPredict[each]\n",
        "                if set(newPredict[each]) != set(oldPredict[each]):\n",
        "                    diffDict[each] = allDict\n",
        "                elif set(newPredict[each]) == set(oldPredict[each]):\n",
        "                    simDict[each] = allDict\n",
        "        diffFrame = pd.DataFrame.from_dict(diffDict)\n",
        "        diffFrame = diffFrame.transpose()\n",
        "        diffFrame.to_csv(base_dir + \"comparison/\" + loc + \"_compfile.csv\", index=False)\n",
        "        simFrame = pd.DataFrame.from_dict(simDict)\n",
        "        simFrame = simFrame.transpose()\n",
        "        simFrame.to_csv(base_dir + \"comparison/\" + loc + \"_similar.csv\", index=False)\n",
        "        \n",
        "\n",
        "        for allele in newPredict.keys():\n",
        "            allDict = {}\n",
        "            allDict[\"Allele\"] = allele\n",
        "            allDict[\"Serologic Assignment\"] = newPredict[allele]\n",
        "            if allele not in oldPredict.keys():\n",
        "                newDict[allele] = allDict\n",
        "        newFrame = pd.DataFrame.from_dict(simDict)\n",
        "        newFrame = newFrame.transpose()\n",
        "        newFrame.to_csv(base_dir + \"comparison/\" + loc + \"_newsies.csv\", index=False)\n",
        "\n",
        "        simLen = len(simFrame)\n",
        "        diffLen = len(diffFrame)\n",
        "        with open(base_dir + \"comparison/\" + loc + \"_concordance.txt\", \"w+\") as fhandle:\n",
        "            fhandle.write(\"HLA-\" +loc+ \" Similar: \" + str(simLen))\n",
        "            fhandle.write(\"HLA-\" +loc+ \" Different: \" + str(diffLen))\n",
        "            concordance = (simLen / (simLen + diffLen)) * 100\n",
        "            concordances[loc] = concordance\n",
        "            fhandle.write(\"HLA-\" +loc+ \" Concordance: \" + str(concordance) + \"%\")\n",
        "            if print_all == \"yes\":\n",
        "                print(\"HLA-\" +loc+ \" Similar: \" + str(simLen))\n",
        "                print(\"HLA-\" +loc+ \" Different: \" + str(diffLen))\n",
        "                print(\"HLA-\" +loc+ \" Concordance: \" + str(concordance) + \"%\")\n",
        "    return concordances\n",
        "\n",
        "#main(print_all=\"yes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-OE6SumgONm5"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "def one_hot_decode(df):\n",
        "\tdf['serology']=''\n",
        "\n",
        "\tfor col in df.columns:\n",
        "\t\tdf.loc[df[col]==1,'serology'] = df['serology']+col+';'\n",
        "\n",
        "\treturn df\n",
        "\n",
        "def fix_data(uniques, data, loc, iset, ident):\n",
        "  sero = {}\n",
        "  for row in data.itertuples(name='Pandas'):\n",
        "    sero[row.allele] = str(row.serology)\n",
        "    #sero[row[1]] = str(row[-1])\n",
        "\t\n",
        "  data = data.drop('serology', axis=1)\n",
        "\n",
        "  for key in sero.keys():\n",
        "    '''\n",
        "  \t# not applicable for old_sets train/test\n",
        "    if (sero[key].find(';') != -1):\n",
        "      sero[key] = sero[key].replace('a','')\n",
        "      sero[key] = sero[key].split(';')\n",
        "    else:\n",
        "      sero[key] = sero[key].replace('a','')\n",
        "      sero[key] = [sero[key]]\n",
        "    '''\n",
        "\n",
        "    #for old_sets train/test\n",
        "    sero[key] = sero[key].split(' ')\n",
        "    \n",
        "    for x in sero[key]:\n",
        "      if (x not in uniques):\n",
        "        uniques.append(x)\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  uniques = list(map(int, uniques))\n",
        "  uniques.sort()\n",
        "  uniques = list(map(str, uniques))\n",
        "  \n",
        "  for y in uniques:\n",
        "    data[y] = 0\n",
        "\n",
        "  one_sero = {}\n",
        "  for key in sero.keys():\n",
        "    one_sero[key] = { some_key : (\"1\" if (some_key in sero[key]) else \"0\")\n",
        "\t\t                  for some_key in uniques }\n",
        "  one_df = pd.DataFrame.from_dict(one_sero)\n",
        "  one_df = one_df.transpose()\n",
        "  one_df.index.name = \"allele\"\n",
        "  data = data.set_index('allele')\n",
        "  data.update(one_df, overwrite=True)\n",
        "  data.to_csv(base_dir + 'randfor/'+iset+'/'+loc+'_'+ident+'.csv', index=True)\n",
        "  return data, uniques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFXrjuPaKmln",
        "outputId": "252a7877-966f-49d8-8a6a-50584bcddce6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting...\n",
            "A\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [00:11<00:47, 11.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [00:30<00:48, 16.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gbiagini/miniconda3/envs/nn-sero/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (1793) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            " 60%|██████    | 3/5 [00:36<00:22, 11.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DQB1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [00:40<00:08,  8.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DRB1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gbiagini/miniconda3/envs/nn-sero/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (812) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "100%|██████████| 5/5 [00:47<00:00,  9.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#RSEED = 0\n",
        "\n",
        "pre_concord = metrics()\n",
        "\n",
        "loci = [\"A\", \"B\", \"C\", \"DQB1\", \"DRB1\"]\n",
        "print(\"Predicting...\")\n",
        "for loc in tqdm(loci):\n",
        "  uniques = []\n",
        "  print(loc)\n",
        "  features = pd.read_csv(base_dir + \"training/\" + loc + \"_train.csv\")\n",
        "  #features['serology'] = features['serology'].apply(lambda x: x.replace('a','').replace(';',' '))\n",
        "  features, sers = fix_data(uniques, features,loc,iset='training',ident='train')\n",
        "  vfeatures = pd.read_csv(base_dir + \"training/\" + loc + \"_validation.csv\")\n",
        "  #vfeatures['serology'] = vfeatures['serology'].apply(lambda x: x.replace('a','').replace(';',' '))\n",
        "  vfeatures, vsers = fix_data(uniques, vfeatures,loc,iset='training',ident='validation')\n",
        "  test = pd.read_csv(base_dir + \"testing/\" + loc + \"_test.csv\")\n",
        "  test = test.drop('serology', axis=1)\n",
        "  test.to_csv(base_dir + 'randfor/testing/'+loc+'_test.csv', index=True)\n",
        "\n",
        "  features = features.append(vfeatures)\n",
        "  labels = np.array(features[sers])\n",
        "  features = features.drop(sers, axis=1)\n",
        "  features = features.reset_index()\n",
        "  indices = features[\"allele\"]\n",
        "  indices = list(indices)\n",
        "  features = features.drop('allele', axis=1)\n",
        "  feature_list = list(features.columns)\n",
        "  n_features = len(feature_list)\n",
        "  maxfeat = int(math.sqrt(n_features))\n",
        "\n",
        "  features = np.array(features)\n",
        "  labels[labels!=labels]='0'\n",
        "  features[features!=features]='0'\n",
        "  features = features.astype(int)\n",
        "  labels = labels.astype(int)\n",
        "\n",
        "  test_idcs = test['allele']\n",
        "  test = test.drop('allele', axis=1)\n",
        "  #print(test.head(100))\n",
        "  test_list = list(test.columns)\n",
        "  test = np.array(test)\n",
        "  test[test!=test]='0'\n",
        "  test = test.astype(int)\n",
        "\n",
        "  forest = RandomForestClassifier(n_estimators=500, bootstrap=True, max_features=maxfeat, n_jobs=-1)\n",
        "  multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)\n",
        "  multi_target_forest.fit(features,labels)\n",
        "  predictions = multi_target_forest.predict(test)\n",
        "\n",
        "  ind_labels = [str(x) for x in sers]\n",
        "  #explainer = lime.lime_tabular.LimeTabularExplainer(features,feature_names=feature_list,class_names=ind_labels,kernel_width=5)\n",
        "  #for rowexp in range(0,2):\n",
        "  #  exp = explainer.explain_instance(test[rowexp], multi_target_forest.predict_proba, num_features=maxfeat)\n",
        "  #  exp.show_in_notebook(show_table=True)\n",
        "\n",
        "  preds_output = pd.DataFrame(predictions, index=test_idcs, columns=ind_labels)\n",
        "  preds_output = one_hot_decode(preds_output)\n",
        "  preds_output = preds_output.drop(ind_labels, axis=1)\n",
        "  preds_output.index.name = 'allele'\n",
        "  preds_output = preds_output.apply(lambda x: str((x['serology'].split(';'))[:-1]), result_type='broadcast', axis=1)\n",
        "  #print(preds_output)\n",
        "  preds_output.to_csv(base_dir + 'predictions/'+loc+'_predictions.csv', index=True)\n",
        "\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ggEURdcKyWC",
        "outputId": "9051b0d9-b8cd-436d-d5ef-45415b32469e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A Concordance:\t\t\t\t92.78%\n",
            "% Change:\t\t\t\t-1.59%\n",
            "B Concordance:\t\t\t\t83.06%\n",
            "% Change:\t\t\t\t0.088%\n",
            "C Concordance:\t\t\t\t67.88%\n",
            "% Change:\t\t\t\t-1.29%\n",
            "DQB1 Concordance:\t\t\t\t87.31%\n",
            "% Change:\t\t\t\t0.362%\n",
            "DRB1 Concordance:\t\t\t\t89.84%\n",
            "% Change:\t\t\t\t-0.09%\n"
          ]
        }
      ],
      "source": [
        "post_concord = metrics()\n",
        "\n",
        "for loc in loci:\n",
        "\tprint(loc + \" Concordance:\\t\\t\\t\\t\" + str(post_concord[loc])[:5] + \"%\")\n",
        "\tchange = post_concord[loc] - pre_concord[loc]\n",
        "\tprint(\"% Change:\\t\\t\\t\\t\" + str(change)[:5] + \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PjeCUnkq02IY"
      },
      "outputs": [
        {
          "ename": "NotFittedError",
          "evalue": "This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-86240b53f1db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n\u001b[1;32m      3\u001b[0m              axis=0)\n\u001b[1;32m      4\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/nn-sero/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfeature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \"\"\"\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         all_importances = Parallel(n_jobs=self.n_jobs,\n",
            "\u001b[0;32m~/miniconda3/envs/nn-sero/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/nn-sero/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ],
      "source": [
        "importances = multi_target_forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in multi_target_forest.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "for f in range(X.shape[1]):\n",
        "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
        "\n",
        "# Plot the impurity-based feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(X.shape[1]), importances[indices],\n",
        "        color=\"r\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), indices)\n",
        "plt.xlim([-1, X.shape[1]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "rf-sero.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('nn-sero': conda)",
      "name": "python392jvsc74a57bd095132b5c7dd088d4c50be6f2c326597d20ac757df5ad8e6ddcb3ea48700d8bdb"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "metadata": {
      "interpreter": {
        "hash": "bab016364d1038d8da29e67805d1a3377d41c5891bd8e13f5bd752f87e562a16"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}